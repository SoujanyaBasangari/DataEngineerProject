{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Prefect Cloud with Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**NOTE**: rrerequsities\n",
    "* created a Prefect Cloud account\n",
    "* set up the appropriate credentials in Saturn\n",
    "* set up a Prefect Cloud agent in Saturn Cloud\n",
    "\n",
    "Details on these prerequisites can be found in [\"Using Prefect Cloud with Saturn Cloud\"](https://saturncloud.io/docs/examples/prefect/prefect_cloud/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "The code in this notebook uses `prefect` for orchestration *(figuring out what to do, and in what order)* and `dask` for execution *(doing the things)*.\n",
    "\n",
    "It relies on the following additional non-builtin libraries:\n",
    "\n",
    "* `pyspark`: data manipulation\n",
    "* `pyspark`: read in data from the server\n",
    "* `dask-saturn`: create and interact with Saturn Cloud `Dask` clusters ([link](https://github.com/saturncloud/dask-saturn))\n",
    "* `prefect-saturn`: register Prefect flows with both Prefect Cloud and have them run on Saturn Cloud Dask clusters ([link](https://github.com/saturncloud/prefect-saturn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T14:42:29.508602Z",
     "iopub.status.busy": "2021-09-30T14:42:29.508311Z",
     "iopub.status.idle": "2021-09-30T14:42:29.513234Z",
     "shell.execute_reply": "2021-09-30T14:42:29.512624Z",
     "shell.execute_reply.started": "2021-09-30T14:42:29.508578Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import prefect\n",
    "import requests\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from prefect import task, Flow, Parameter\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as F\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from prefect import task, Parameter, Flow\n",
    "from prefect.schedules import IntervalSchedule\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from prefect_saturn import PrefectCloudIntegration\n",
    "\n",
    "PREFECT_CLOUD_PROJECT_NAME = 'TLCData' #os.environ[\"TLCData\"]\n",
    "SATURN_USERNAME = os.environ[\"SATURN_USERNAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with Prefect Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T14:48:56.031976Z",
     "iopub.status.busy": "2021-09-30T14:48:56.031685Z",
     "iopub.status.idle": "2021-09-30T14:48:57.563652Z",
     "shell.execute_reply": "2021-09-30T14:48:57.562977Z",
     "shell.execute_reply.started": "2021-09-30T14:48:56.031953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You logged in with an API key using the `--token` flag which is deprecated. Please use `--key` instead.\u001b[0m\n",
      "\u001b[32mLogged in to Prefect Cloud tenant \"basangarisoujanya@gmail.com's Account\" (basangarisoujanya-gmail-com-s-account)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prefect auth login --key ${'mTEaxoDLi5NnI4DUWBvZvw'}\n",
    "#!prefect auth login -t <'mTEaxoDLi5NnI4DUWBvZvw'>\n",
    "\n",
    "! prefect auth login -t mTEaxoDLi5NnI4DUWBvZvw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Create a Prefect Cloud Project\n",
    "\n",
    "Prefect Cloud organizes flows within workspaces called \"projects\". Before you can register a flow with Prefect Cloud, it's necessary to create a project if you don't have one yet.\n",
    "\n",
    "The code below will create a new project in whatever Prefect Cloud tenant you're authenticated with. If that project already exists, this code does not have any side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:09:53.721035Z",
     "iopub.status.busy": "2021-09-30T15:09:53.720742Z",
     "iopub.status.idle": "2021-09-30T15:09:53.999654Z",
     "shell.execute_reply": "2021-09-30T15:09:53.999130Z",
     "shell.execute_reply.started": "2021-09-30T15:09:53.721012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'121c084a-5c03-498d-9a5f-fcc5042716f1'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = prefect.Client()\n",
    "client.create_project(project_name='PREFECT_CLOUD_PROJECT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Define Tasks\n",
    "\n",
    "`prefect` refers to a workload as a \"flow\", which comprises multiple individual things to do called \"tasks\". From [the Prefect docs](https://docs.prefect.io/core/concepts/tasks.html):\n",
    "\n",
    "* `get_trial_id()`: assign a unique ID to each run\n",
    "* `get_trial_id()`: assign a unique ID to each run\n",
    "* `extract()`: extract data from cloud(where csv for three years are stored)\n",
    "* `transform()`: transform dataset to column oriented and row oriented \n",
    "* `load()`:  merge datasets and load in database\n",
    "* `get_trial_summary()`: collect all evaluation metrics in one object\n",
    "* `write_trial_summary()`: write trial results somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:09:55.152646Z",
     "iopub.status.busy": "2021-09-30T15:09:55.152420Z",
     "iopub.status.idle": "2021-09-30T15:09:55.165162Z",
     "shell.execute_reply": "2021-09-30T15:09:55.164602Z",
     "shell.execute_reply.started": "2021-09-30T15:09:55.152625Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from prefect import task, Flow, Parameter\n",
    "\n",
    "@task(max_retries=10, retry_delay=timedelta(seconds=10))\n",
    "def extract(url: str) -> dict:\n",
    "    try :\n",
    "        ypath = url+\"yellow_tripdata_*.csv\"\n",
    "        gpath = url+\"green_tripdata_*.csv\"\n",
    "      \n",
    "        taxi_schema = StructType([StructField(\"VendorID\", IntegerType(), False),\n",
    "                                  StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "                                  StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "                                  StructField(\"store_and_fwd_flag\", StringType(), False),\n",
    "                                  StructField(\"RatecodeID\", IntegerType(), False),\n",
    "                                  StructField(\"PULocationID\", IntegerType(), False),\n",
    "                                  StructField(\"DOLocationID\", IntegerType(), False),\n",
    "                                  StructField(\"passenger_count\", IntegerType(), False),\n",
    "                                  StructField(\"trip_distance\", FloatType(), False),\n",
    "                                  StructField(\"fare_amount\", FloatType(), False),\n",
    "                                  StructField(\"extra\", FloatType(), False),\n",
    "                                  StructField(\"mta_tax\", FloatType(), False),\n",
    "                                  StructField(\"tip_amount\", FloatType(), False),\n",
    "                                  StructField(\"tolls_amount\", FloatType(), False),\n",
    "                                  StructField(\"ehail_fee\", FloatType(), False),\n",
    "                                  StructField(\"improvement_surcharge\", FloatType(), False),\n",
    "                                  StructField(\"total_amount\", FloatType(), False),\n",
    "                                  StructField(\"payment_type\", IntegerType(), False),\n",
    "                                  StructField(\"trip_type\", IntegerType(), False)])\n",
    "\n",
    "        yellow_df = spark.read.option(\"header\", True)\\\n",
    "        .schema(taxi_schema) \\\n",
    "        .csv(ypath)\\\n",
    "        .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "        .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n",
    "        .withColumn(\"taxi_type\", lit(\"yellow\")) \\\n",
    "        .withColumn(\"ehail_fee\", lit(0.0)) \n",
    "   \n",
    "    \n",
    "        green_df = spark.read.option(\"header\", True)\\\n",
    "        .schema(taxi_schema) \\\n",
    "        .csv(gpath) \\\n",
    "        .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "        .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n",
    "        .withColumn(\"taxi_type\", lit(\"green\"))\n",
    "\n",
    "    except:\n",
    "        raise Exception('No data fetched!')\n",
    "    \n",
    "    return yellow_df,green_df\n",
    "\n",
    "\n",
    "@task\n",
    "def transform(yellow_df: pd.DataFrame,green_df: pd.DataFrame):\n",
    "    #Add hour column\n",
    "    yellow_df = yellow_df.withColumn(\"pickup_hour\", F.from_unixtime(F.unix_timestamp(col(\"pickup_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    green_df = green_df.withColumn(\"pickup_hour\", F.from_unixtime(F.unix_timestamp(col(\"pickup_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    yellow_df = yellow_df.withColumn(\"dropoff_hour\", F.from_unixtime(F.unix_timestamp(col(\"dropoff_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    green_df = green_df.withColumn(\"dropoff_hour\", F.from_unixtime(F.unix_timestamp(col(\"dropoff_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    taxi_df = yellow_df.union(green_df)\n",
    "    taxi_schema = StructType(\n",
    "      [StructField(\"VendorID\", IntegerType(), False),\n",
    "      StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "      StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "      StructField(\"store_and_fwd_flag\", StringType(), False),\n",
    "      StructField(\"RatecodeID\", IntegerType(), False),\n",
    "      StructField(\"PULocationID\", IntegerType(), False),\n",
    "      StructField(\"DOLocationID\", IntegerType(), False),\n",
    "      StructField(\"passenger_count\", IntegerType(), False),\n",
    "      StructField(\"trip_distance\", FloatType(), False),\n",
    "      StructField(\"fare_amount\", FloatType(), False),\n",
    "      StructField(\"extra\", FloatType(), False),\n",
    "      StructField(\"mta_tax\", FloatType(), False),\n",
    "      StructField(\"tip_amount\", FloatType(), False),\n",
    "      StructField(\"tolls_amount\", FloatType(), False),\n",
    "      StructField(\"ehail_fee\", FloatType(), False),\n",
    "      StructField(\"improvement_surcharge\", FloatType(), False),\n",
    "      StructField(\"total_amount\", FloatType(), False),\n",
    "      StructField(\"payment_type\", IntegerType(), False),\n",
    "      StructField(\"trip_type\", IntegerType(), False),\n",
    "      StructField(\"taxi_type\", IntegerType(), False),\n",
    "      StructField(\"pickup_hour\", IntegerType(), False),\n",
    "      StructField(\"dropoff_hour\", IntegerType(), False)])\n",
    "    \n",
    "    taxi_df.write.option(\"schema\",taxi_schema).mode('append').parquet(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.parquet\")\n",
    "\n",
    "    avro_schema = { \"type\": \"record\",\n",
    "    \"name\":\"avro_schema\",\n",
    "    \"type\":\"record\",\n",
    "        \"fields\":[\n",
    "            {\"type\":\"int\", \"name\":\"VendorID\"},\n",
    "            {\"type\":\"datetime\", \"name\":\"pickup_datetime\"}\n",
    "            {\"type\":\"datetime\", \"name\":\"dropoff_datetime\"}\n",
    "            {\"type\":\"string\", \"name\":\"store_and_fwd_flag\"}\n",
    "            {\"type\":\"int\", \"name\":\"RatecodeID\"}\n",
    "            {\"type\":\"int\", \"name\":\"PULocationID\"}\n",
    "            {\"type\":\"int\", \"name\":\"DOLocationID\"}\n",
    "            {\"type\":\"int\", \"name\":\"passenger_count\"}\n",
    "            {\"type\":\"float\", \"name\":\"trip_distance\"}\n",
    "            {\"type\":\"float\", \"name\":\"fare_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"extra\"}\n",
    "            {\"type\":\"float\", \"name\":\"mta_tax\"}\n",
    "            {\"type\":\"float\", \"name\":\"tip_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"tolls_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"ehail_fee\"}\n",
    "            {\"type\":\"float\", \"name\":\"improvement_surcharge\"}\n",
    "            {\"type\":\"float\", \"name\":\"total_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"payment_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"trip_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"taxi_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"pickup_hour\"}\n",
    "            {\"type\":\"float\", \"name\":\"dropoff_hour\"}\n",
    "        ]\n",
    "     }\n",
    "    \n",
    "    taxi_df.write.option(\"forceSchema\", avro_schema).save(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.avro\")\n",
    "   \n",
    "    # taxi_df_parquet = spark.read.parquet(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.parquet\")\n",
    "    \n",
    "    # taxi_df_avro = sqlContext.read.format(\"com.databricks.spark.avro\").load(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.avro\")\n",
    "    \n",
    "    return taxi_df\n",
    "\n",
    "\n",
    "@task\n",
    "def load(taxi_df: pd.DataFrame, path: str) -> None:\n",
    "    \n",
    "    # If output is needed in csv \n",
    "    taxi_df.write.csv('output.csv')\n",
    "    #set variable to be used to connect the database\n",
    "    database = \"TestDB\"\n",
    "    table = \"dbo.tbl_spark_df\"\n",
    "     #write the dataframe into a sql table\n",
    "    taxi_df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://localhost/SQLEXPRESS;databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()\n",
    "\n",
    "    #for updating\n",
    "    #taxi_df.write.mode(SaveMode.Append).jdbc(JDBCurl,mySqlTable,connectionProperties)\n",
    "    \n",
    "@task\n",
    "def get_trial_id() -> str:\n",
    "    #Generate a unique identifier for this trial.\n",
    "\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "@task\n",
    "def get_trial_summary(trial_id: str, taxi_df: pd.DataFrame) -> dict:\n",
    "    out = {\"id\": trial_id}\n",
    "    out[\"data\"] = {\n",
    "        \"num_obs\": taxi_df.shape[0],\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "@task(log_stdout=True)\n",
    "def write_trial_summary(trial_summary: str):\n",
    "    \"\"\"\n",
    "    Write out a summary of the file. Currently just logs back to the\n",
    "    Prefect logger\n",
    "    \"\"\"\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "    logger.info(json.dumps(trial_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Construct a Flow\n",
    "\n",
    "Now that all of the task logic has been defined, the next step is to compose those tasks into a \"flow\". From [the Prefect docs](https://docs.prefect.io/core/concepts/flows.html):\n",
    "\n",
    "Because we want this job to run on a schedule, the code below provides one additional argument to `Flow()`, a special \"schedule\" object. In this case, the code below says \"run this flow once every 24 hours\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:09:56.559162Z",
     "iopub.status.busy": "2021-09-30T15:09:56.558865Z",
     "iopub.status.idle": "2021-09-30T15:09:56.562527Z",
     "shell.execute_reply": "2021-09-30T15:09:56.561810Z",
     "shell.execute_reply.started": "2021-09-30T15:09:56.559137Z"
    }
   },
   "outputs": [],
   "source": [
    "schedule = IntervalSchedule(interval=timedelta(hours=24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: `prefect` flows do not have to be run on a schedule. To test a single run, just omit `schedule` from the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:11:08.270063Z",
     "iopub.status.busy": "2021-09-30T15:11:08.269693Z",
     "iopub.status.idle": "2021-09-30T15:11:08.276965Z",
     "shell.execute_reply": "2021-09-30T15:11:08.276359Z",
     "shell.execute_reply.started": "2021-09-30T15:11:08.270024Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with Flow(f\"{SATURN_USERNAME}-tlcdata\", schedule=schedule) as flow:\n",
    "    param_url = Parameter(name='p_url', required=True)\n",
    "    \n",
    "    yellow_df,green_df = extract(url=param_url)\n",
    "    taxi_df = transform(yellow_df,green_df)\n",
    "    load(data=taxi_df, path=f'C:/Users/Soujanya/users_{int(datetime.now().timestamp())}.csv')\n",
    "    batch_size = Parameter(\"batch-size\", default=1000)\n",
    "    trial_id = get_trial_id()\n",
    "\n",
    "    # get trial summary in a string\n",
    "    trial_summary = get_trial_summary(\n",
    "        trial_id=trial_id,\n",
    "        input_df=taxi_df,\n",
    "    )\n",
    "\n",
    "    # store trial summary\n",
    "    trial_complete = write_trial_summary(trial_summary)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all of the work defined in tasks and arranged within a flow, but none of the tasks have run yet. In the next section, we'll do that using `Dask`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Register with Prefect Cloud\n",
    "\n",
    "Now that the business logic of the flow is complete, we can add information that Saturn will need to know to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:11:10.310754Z",
     "iopub.status.busy": "2021-09-30T15:11:10.310403Z",
     "iopub.status.idle": "2021-09-30T15:11:10.315086Z",
     "shell.execute_reply": "2021-09-30T15:11:10.314453Z",
     "shell.execute_reply.started": "2021-09-30T15:11:10.310729Z"
    }
   },
   "outputs": [],
   "source": [
    "integration = PrefectCloudIntegration(prefect_cloud_project_name=PREFECT_CLOUD_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run `register_flow_with_saturn().\n",
    "\n",
    "`register_flow_with_saturn()` does a few important things:\n",
    "    \n",
    "The code below also customizes the Dask cluster used when executing the flow.\n",
    "\n",
    "* `n_workers = 3`: use 3 workers\n",
    "* `worker_size =\"xlarge\"`: each worker has 2 CPU cores and 16 GB RAM\n",
    "    - **NOTE**: you can find the full list of sizes with `prefect_saturn.describe_sizes()`\n",
    "* `worker_is_spot = False`: don't use spot instances for workers\n",
    "\n",
    "**NOTE:** dask clusters associated with prefect cloud flows will be autoclosed when the flow run completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:11:52.311404Z",
     "iopub.status.busy": "2021-09-30T15:11:52.311085Z",
     "iopub.status.idle": "2021-09-30T15:11:52.484642Z",
     "shell.execute_reply": "2021-09-30T15:11:52.484044Z",
     "shell.execute_reply.started": "2021-09-30T15:11:52.311379Z"
    }
   },
   "outputs": [],
   "source": [
    "flow = integration.register_flow_with_saturn(\n",
    "    flow=flow,\n",
    "    dask_cluster_kwargs={\n",
    "        \"n_workers\": 3,\n",
    "        \"worker_size\": \"xlarge\",\n",
    "        \"scheduler_size\": \"medium\",\n",
    "        \"worker_is_spot\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "flow.run(parameters={\n",
    "        'p_url': 'https://jsonplaceholder.typicode.com/DOES_NOT_EXIST'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step necessary is to \"register\" the flow with Prefect Cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T15:11:13.093672Z",
     "iopub.status.busy": "2021-09-30T15:11:13.093248Z",
     "iopub.status.idle": "2021-09-30T15:11:13.106575Z",
     "shell.execute_reply": "2021-09-30T15:11:13.105640Z",
     "shell.execute_reply.started": "2021-09-30T15:11:13.093644Z"
    }
   },
   "outputs": [],
   "source": [
    "flow.register(project_name=\"PREFECT_CLOUD_PROJECT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Run the flow\n",
    "\n",
    "If you want to run the flow immediately, navigate to the flow in the Prefect Cloud UI and click \"Quick Run\", or open a terminal and run the code below.\n",
    "\n",
    "```shell\n",
    "prefect auth login --key ${PREFECT_USER_TOKEN}\n",
    "prefect run flow \\\n",
    "    --name ${SATURN_USERNAME}-ticket-model-evaluation \\\n",
    "    --project ${PREFECT_CLOUD_PROJECT_NAME}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
