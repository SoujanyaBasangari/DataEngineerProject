{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a Scheduled Data Pipeline with Prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "The code in this notebook uses `prefect` for orchestration *(figuring out what to do, and in what order)* and `dask` for execution *(doing the things)*.\n",
    "\n",
    "It relies on the following additional non-builtin libraries:\n",
    "\n",
    "\n",
    "* `pyspark`: data manipulation\n",
    "* `pyspark`: read in data from the server\n",
    "* `dask-saturn`: create and interact with Saturn Cloud `Dask` clusters ([link](https://github.com/saturncloud/dask-saturn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import prefect\n",
    "import requests\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from prefect import task, Flow, Parameter\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as F\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from prefect import task, Parameter, Flow\n",
    "from prefect.engine.executors.dask import DaskExecutor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from dask_saturn import SaturnCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Define Tasks\n",
    "\n",
    "`prefect` refers to a workload as a \"flow\", which comprises multiple individual things to do called \"tasks\". From [the Prefect docs](https://docs.prefect.io/core/concepts/tasks.html):\n",
    "\n",
    "> A task is like a function: it optionally takes inputs, performs an action, and produces an optional result.\n",
    "\n",
    "The goal of this notebooks flow is to evaluate, on an ongoing basis, the performance of a model that predicts time-to-close for tickets in an IT support system.\n",
    "\n",
    "That can be broken down into the following tasks\n",
    "\n",
    "* `get_trial_id()`: assign a unique ID to each run\n",
    "* `extract()`: extract data from cloud(where csv for three years are stored)\n",
    "* `transform()`: transform dataset to column oriented and row oriented \n",
    "* `load()`:  merge datasets and load in database\n",
    "* `get_trial_summary()`: collect all evaluation metrics in one object\n",
    "* `write_trial_summary()`: write trial results somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from prefect import task, Flow, Parameter\n",
    "\n",
    "@task(max_retries=10, retry_delay=timedelta(seconds=10))\n",
    "def extract(url: str) -> dict:\n",
    "    try :\n",
    "        ypath = url+\"yellow_tripdata_*.csv\"\n",
    "        gpath = url+\"green_tripdata_*.csv\"\n",
    "      \n",
    "        taxi_schema = StructType([StructField(\"VendorID\", IntegerType(), False),\n",
    "                                  StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "                                  StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "                                  StructField(\"store_and_fwd_flag\", StringType(), False),\n",
    "                                  StructField(\"RatecodeID\", IntegerType(), False),\n",
    "                                  StructField(\"PULocationID\", IntegerType(), False),\n",
    "                                  StructField(\"DOLocationID\", IntegerType(), False),\n",
    "                                  StructField(\"passenger_count\", IntegerType(), False),\n",
    "                                  StructField(\"trip_distance\", FloatType(), False),\n",
    "                                  StructField(\"fare_amount\", FloatType(), False),\n",
    "                                  StructField(\"extra\", FloatType(), False),\n",
    "                                  StructField(\"mta_tax\", FloatType(), False),\n",
    "                                  StructField(\"tip_amount\", FloatType(), False),\n",
    "                                  StructField(\"tolls_amount\", FloatType(), False),\n",
    "                                  StructField(\"ehail_fee\", FloatType(), False),\n",
    "                                  StructField(\"improvement_surcharge\", FloatType(), False),\n",
    "                                  StructField(\"total_amount\", FloatType(), False),\n",
    "                                  StructField(\"payment_type\", IntegerType(), False),\n",
    "                                  StructField(\"trip_type\", IntegerType(), False)])\n",
    "\n",
    "        yellow_df = spark.read.option(\"header\", True)\\\n",
    "        .schema(taxi_schema) \\\n",
    "        .csv(ypath)\\\n",
    "        .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "        .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n",
    "        .withColumn(\"taxi_type\", lit(\"yellow\")) \\\n",
    "        .withColumn(\"ehail_fee\", lit(0.0)) \n",
    "   \n",
    "    \n",
    "        green_df = spark.read.option(\"header\", True)\\\n",
    "        .schema(taxi_schema) \\\n",
    "        .csv(gpath) \\\n",
    "        .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "        .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n",
    "        .withColumn(\"taxi_type\", lit(\"green\"))\n",
    "\n",
    "    except:\n",
    "        raise Exception('No data fetched!')\n",
    "    \n",
    "    return yellow_df,green_df\n",
    "\n",
    "\n",
    "@task\n",
    "def transform(yellow_df: pd.DataFrame,green_df: pd.DataFrame):\n",
    "    #Add hour column\n",
    "    yellow_df = yellow_df.withColumn(\"pickup_hour\", F.from_unixtime(F.unix_timestamp(col(\"pickup_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    green_df = green_df.withColumn(\"pickup_hour\", F.from_unixtime(F.unix_timestamp(col(\"pickup_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    yellow_df = yellow_df.withColumn(\"dropoff_hour\", F.from_unixtime(F.unix_timestamp(col(\"dropoff_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "    green_df = green_df.withColumn(\"dropoff_hour\", F.from_unixtime(F.unix_timestamp(col(\"dropoff_datetime\"),\"yyyy-MM-dd hh:mm:ss\"),\"yyyy-MM-dd hh:00:00\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    taxi_df = yellow_df.union(green_df)\n",
    "    taxi_schema = StructType(\n",
    "      [StructField(\"VendorID\", IntegerType(), False),\n",
    "      StructField(\"pickup_datetime\", TimestampType(), False),\n",
    "      StructField(\"dropoff_datetime\", TimestampType(), False),\n",
    "      StructField(\"store_and_fwd_flag\", StringType(), False),\n",
    "      StructField(\"RatecodeID\", IntegerType(), False),\n",
    "      StructField(\"PULocationID\", IntegerType(), False),\n",
    "      StructField(\"DOLocationID\", IntegerType(), False),\n",
    "      StructField(\"passenger_count\", IntegerType(), False),\n",
    "      StructField(\"trip_distance\", FloatType(), False),\n",
    "      StructField(\"fare_amount\", FloatType(), False),\n",
    "      StructField(\"extra\", FloatType(), False),\n",
    "      StructField(\"mta_tax\", FloatType(), False),\n",
    "      StructField(\"tip_amount\", FloatType(), False),\n",
    "      StructField(\"tolls_amount\", FloatType(), False),\n",
    "      StructField(\"ehail_fee\", FloatType(), False),\n",
    "      StructField(\"improvement_surcharge\", FloatType(), False),\n",
    "      StructField(\"total_amount\", FloatType(), False),\n",
    "      StructField(\"payment_type\", IntegerType(), False),\n",
    "      StructField(\"trip_type\", IntegerType(), False),\n",
    "      StructField(\"taxi_type\", IntegerType(), False),\n",
    "      StructField(\"pickup_hour\", IntegerType(), False),\n",
    "      StructField(\"dropoff_hour\", IntegerType(), False)])\n",
    "    \n",
    "    taxi_df.write.option(\"schema\",taxi_schema).mode('append').parquet(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.parquet\")\n",
    "\n",
    "    avro_schema = { \"type\": \"record\",\n",
    "    \"name\":\"avro_schema\",\n",
    "    \"type\":\"record\",\n",
    "        \"fields\":[\n",
    "            {\"type\":\"int\", \"name\":\"VendorID\"},\n",
    "            {\"type\":\"datetime\", \"name\":\"pickup_datetime\"}\n",
    "            {\"type\":\"datetime\", \"name\":\"dropoff_datetime\"}\n",
    "            {\"type\":\"string\", \"name\":\"store_and_fwd_flag\"}\n",
    "            {\"type\":\"int\", \"name\":\"RatecodeID\"}\n",
    "            {\"type\":\"int\", \"name\":\"PULocationID\"}\n",
    "            {\"type\":\"int\", \"name\":\"DOLocationID\"}\n",
    "            {\"type\":\"int\", \"name\":\"passenger_count\"}\n",
    "            {\"type\":\"float\", \"name\":\"trip_distance\"}\n",
    "            {\"type\":\"float\", \"name\":\"fare_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"extra\"}\n",
    "            {\"type\":\"float\", \"name\":\"mta_tax\"}\n",
    "            {\"type\":\"float\", \"name\":\"tip_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"tolls_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"ehail_fee\"}\n",
    "            {\"type\":\"float\", \"name\":\"improvement_surcharge\"}\n",
    "            {\"type\":\"float\", \"name\":\"total_amount\"}\n",
    "            {\"type\":\"float\", \"name\":\"payment_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"trip_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"taxi_type\"}\n",
    "            {\"type\":\"float\", \"name\":\"pickup_hour\"}\n",
    "            {\"type\":\"float\", \"name\":\"dropoff_hour\"}\n",
    "        ]\n",
    "     }\n",
    "    \n",
    "    taxi_df.write.option(\"forceSchema\", avro_schema).save(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.avro\")\n",
    "   \n",
    "    # taxi_df_parquet = spark.read.parquet(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.parquet\")\n",
    "    \n",
    "    # taxi_df_avro = sqlContext.read.format(\"com.databricks.spark.avro\").load(\"https://cloud.uni-koblenz.de/s/tTcoPwsBdoXnWcG/parquet/taxi_df.avro\")\n",
    "    \n",
    "    return taxi_df\n",
    "\n",
    "\n",
    "@task\n",
    "def load(taxi_df: pd.DataFrame, path: str) -> None:\n",
    "    \n",
    "    # If output is needed in csv \n",
    "    taxi_df.write.csv('output.csv')\n",
    "    #set variable to be used to connect the database\n",
    "    database = \"TestDB\"\n",
    "    table = \"dbo.tbl_spark_df\"\n",
    "     #write the dataframe into a sql table\n",
    "    taxi_df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://localhost/SQLEXPRESS;databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()\n",
    "\n",
    "    #for updating\n",
    "    #taxi_df.write.mode(SaveMode.Append).jdbc(JDBCurl,mySqlTable,connectionProperties)\n",
    "    \n",
    "@task\n",
    "def get_trial_id() -> str:\n",
    "    #Generate a unique identifier for this trial.\n",
    "\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "@task\n",
    "def get_trial_summary(trial_id: str, taxi_df: pd.DataFrame) -> dict:\n",
    "    out = {\"id\": trial_id}\n",
    "    out[\"data\"] = {\n",
    "        \"num_obs\": taxi_df.shape[0],\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "@task(log_stdout=True)\n",
    "def write_trial_summary(trial_summary: str):\n",
    "    \"\"\"\n",
    "    Write out a summary of the file. Currently just logs back to the\n",
    "    Prefect logger\n",
    "    \"\"\"\n",
    "    logger = prefect.context.get(\"logger\")\n",
    "    logger.info(json.dumps(trial_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Construct a Flow\n",
    "\n",
    "Now that all of the task logic has been defined, the next step is to compose those tasks into a \"flow\". From [the Prefect docs](https://docs.prefect.io/core/concepts/flows.html):\n",
    "\n",
    "> A Flow is a container for Tasks. It represents an entire workflow or application by describing the dependencies between tasks.\n",
    "\n",
    "> Flows are DAGs, or \"directed acyclic graphs.\" This is a mathematical way of describing certain organizational principles:\n",
    "\n",
    "> * A graph is a data structure that uses \"edges\" to connect \"nodes.\" Prefect models each Flow as a graph in which Task dependencies are modeled by Edges.\n",
    "> * A directed graph means that edges have a start and an end: when two tasks are connected, one of them unambiguously runs first and the other one runs second.\n",
    "> * An acyclic directed graph has no circular dependencies: if you walk through the graph, you will never revisit a task you've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with Flow(f\"{SATURN_USERNAME}-tlcdata\", schedule=schedule) as flow:\n",
    "    param_url = Parameter(name='p_url', required=True)\n",
    "    \n",
    "    yellow_df,green_df = extract(url=param_url)\n",
    "    taxi_df = transform(yellow_df,green_df)\n",
    "    load(data=taxi_df, path=f'C:/Users/Soujanya/users_{int(datetime.now().timestamp())}.csv')\n",
    "    batch_size = Parameter(\"batch-size\", default=1000)\n",
    "    trial_id = get_trial_id()\n",
    "\n",
    "    # get trial summary in a string\n",
    "    trial_summary = get_trial_summary(\n",
    "        trial_id=trial_id,\n",
    "        input_df=taxi_df,\n",
    "    )\n",
    "\n",
    "    # store trial summary\n",
    "    trial_complete = write_trial_summary(trial_summary)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all of the work defined in tasks and arranged within a flow, but none of the tasks have run yet. In the next section, we'll do that using `Dask`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Run the flow with Dask\n",
    "\n",
    "If you run `flow.visualize()` on the code above, you'll see that this flow is not linear. Some tasks are independent of others, and can be run at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dask` is a framework that runs graphs like the one above. Saturn makes it easy to create and manage Dask clusters. The code below will create a Dask cluster with two workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to tell `prefect` to use this `Dask` cluster to do work! This is done with something called an \"executor\". From the [Prefect docs]():\n",
    "\n",
    "> Prefect Executors implement the logic for how Tasks are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = DaskExecutor(\n",
    "    cluster_class=SaturnCluster,\n",
    "    cluster_kwargs={\"n_workers\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the flow! The code below will pass all of the tasks to the `Dask` cluster you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.run(executor=executor)\n",
    "flow.run(parameters={\n",
    "        'p_url': 'https://cloud.uni-koblenz.de/apps/files/DOES_NOT_EXIST'\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
